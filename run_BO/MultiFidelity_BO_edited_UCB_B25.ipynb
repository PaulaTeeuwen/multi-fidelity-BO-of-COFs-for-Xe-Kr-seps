{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d087dd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multi-fidelity Bayesian optimization (MFBO) of COFs for Xe/Kr separations.\n",
    "1. We have a set of COFs from a database. Each COF is characterized by a feature vector $$x \\in \\mathcal{X} \\subset R^d$$ were d=14.\n",
    "\n",
    "2. We have **two different types** of simulations to calculate **the same material property**, the adsorptive Xe/Kr selectivity $S_{Xe/Kr}$. However, we only have a single objective: to maximize the high-fidelity selectivity. \n",
    "$$\\arg\\max_{x \\in \\mathcal{X}}[S^{(\\ell=\\text{high})}_{Xe/Kr}(x)]$$\n",
    "\n",
    "3. Multi-Fidelity options: \n",
    "    1. low-fidelity  => Henry coefficient calculation - MC integration: $S_{Xe/Kr}^{\\text{low}} = \\frac{H_{Xe}}{H_{Kr}}$\n",
    "    2. high-fidelity => GCMC mixture simulation - 80:20 (Kr:Xe) at 298 K and 1.0 bar: $S_{Xe/Kr}^{\\text{high}} = \\frac{n_{Xe} / n_{Kr}}{y_{Xe}/y_{Kr}}$\n",
    "\n",
    "\n",
    "3. We will initialize the surrogate model with a few (3) COFs with simulations under **both** fidelities.\n",
    "    1. The fist COF will be the one closest to the center of the normalized feature space\n",
    "    2. The rest will be chosen to maximize diversity of the training set\n",
    "\n",
    "\n",
    "4. Model:\n",
    "    1. Botorch GP surrogate model: [SingleTaskMultiFidelityGP](https://botorch.org/api/models.html#module-botorch.models.gp_regression_fidelity) or [FixedNoiseMultiFidelityGP](https://botorch.org/api/models.html#botorch.models.gp_regression_fidelity.FixedNoiseMultiFidelityGP)\n",
    "        - Needed to use [this](https://botorch.org/api/optim.html#module-botorch.optim.fit) optimizer to correct matrix jitter\n",
    "    2. We  use the augmented-EI (aEI) acquisition function from [here](https://link.springer.com/content/pdf/10.1007/s00158-005-0587-0.pdf)\n",
    "\n",
    "\n",
    "-  Helpful [tutorial](https://botorch.org/tutorials/discrete_multi_fidelity_bo) for a similar BoTorch Model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc999430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install botorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "from botorch.models import SingleTaskMultiFidelityGP\n",
    "from botorch.acquisition.analytic import ExpectedImprovement\n",
    "from botorch.acquisition.analytic import UpperConfidenceBound\n",
    "from botorch.acquisition.analytic import ProbabilityOfImprovement\n",
    "from botorch.acquisition.analytic import PosteriorMean\n",
    "\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "#from botorch import fit_gpytorch_model \n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "#from botorch.optim.fit import fit_gpytorch_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import h5py # for .jld2 files\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f4355-0bd1-430d-9c4a-585c4ea7c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  figure settings \n",
    "###\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks', palette='Set2', font_scale=1.5, rc={\"lines.linewidth\": 3})\n",
    "sns.despine()\n",
    "# plt.rcParams.update({'font.size': 16})\n",
    "# plt.rcParams['figure.dpi'] = 1200 # 600-1200 for paper quality\n",
    "\n",
    "save_figures = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285c4bb-7658-400d-9d40-9ab31fc7b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_fidelities = [1/3, 2/3] # set of discrete fidelities (in ascending order) to select from this cannot be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fb687",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c587c-20a0-4e3a-9424-f4f42d727c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_study_flag = False # make features have no information for a baseline. to gauge feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe6549-4658-456a-8b36-c0266e71a891",
   "metadata": {},
   "source": [
    "first, the targets (simulated adsorption) and features of the COFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2740b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(\"./targets_and_raw_features.jld2\", \"r\")\n",
    "\n",
    "xtal_names = file['COFs'][:]\n",
    "\n",
    "feature_names = file['feature_names'][:]\n",
    "feature_names = [fn.decode() for fn in feature_names]\n",
    "\n",
    "# feature matrix\n",
    "X = torch.from_numpy(np.transpose(file[\"X\"][:]))\n",
    "\n",
    "if ablation_study_flag:\n",
    "    # shuffle columns\n",
    "    for j in range(X.size()[1]):\n",
    "        shuffled_row_ids = torch.randperm(X.size()[0])\n",
    "        X[:, j] = X[shuffled_row_ids, j]\n",
    "\n",
    "# simulation data\n",
    "y = [torch.from_numpy(np.transpose(file[\"henry_y\"][:])), \n",
    "     torch.from_numpy(np.transpose(file[\"gcmc_y\"][:]))]  \n",
    "\n",
    "print(\"top COF = \", xtal_names[np.argmax(y[1])])\n",
    "# associated simulation costs\n",
    "cost = [np.transpose(file[\"henry_total_elapsed_time\"][:]), # [min]\n",
    "        np.transpose(file[\"gcmc_elapsed_time\"][:])]        # [min]\n",
    "\n",
    "# total number of COFs in data set\n",
    "nb_COFs = X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f19044-cd28-4237-9c8e-51aeab842780",
   "metadata": {},
   "source": [
    "second, the COFs to initialize the surrogate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f38094-06f5-494f-b61e-8ead0fc3b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cof_ids_file = pickle.load(open('../search_results/initializing_cof_ids_normalized.pkl', 'rb'))\n",
    "\n",
    "init_cof_ids = init_cof_ids_file['init_cof_ids']\n",
    "\n",
    "# total number of BO searches to run = number of initializing sets\n",
    "nb_runs = len(init_cof_ids)\n",
    "if ablation_study_flag:\n",
    "    nb_runs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58451aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../search_results/initializing_cof_ids_normalized.pkl', 'rb') as f:\n",
    "    init_file = pickle.load(f)\n",
    "\n",
    "print(init_file.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b92068",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cof_ids = init_file['init_cof_ids']\n",
    "print(type(init_cof_ids))\n",
    "print(len(init_cof_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, init_set in enumerate(init_cof_ids):\n",
    "    print(f\"Initial training set for run {i}:\")\n",
    "    print(init_set)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a109d-4ad5-4ee1-a586-dd01083d6b8e",
   "metadata": {},
   "source": [
    "some tests on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9edb9b-9ca3-4b04-aa63-4ce2637a9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, f in enumerate(feature_names):\n",
    "    print(\"{}: {}\".format(i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbe7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_COF = b'19060N2_ddec.cif' # random COF\n",
    "\n",
    "id_rnd_COF = np.where(xtal_names == rnd_COF)[0]\n",
    "\n",
    "# does the low-fidelity selectivity match that manually read from the simulation output file?\n",
    "assert np.isclose(y[0][id_rnd_COF].item(), 722.409 / 202.085)\n",
    "\n",
    "# does the high-fidelity selectivity match that manually read from the simulation output file?\n",
    "assert np.isclose(y[1][id_rnd_COF].item(), (6.1558810248879325 / 6.842906773660418) / (20/80))\n",
    "\n",
    "# manually check some features\n",
    "if not ablation_study_flag:\n",
    "    # ASA_m^2/g = 4363.81 in Zeo++ output file. this better match!\n",
    "    assert np.isclose(X[id_rnd_COF, 2].item(), 4363.81)\n",
    "\n",
    "    # mol fraction of N = 0.04807692307692308 in the xtal. this better match!\n",
    "    assert np.isclose(X[id_rnd_COF, 9].item(), 0.04807692307692308)\n",
    "    \n",
    "    # sum of mol frac's = 1\n",
    "    assert X[id_rnd_COF, 4:].sum().item() == 1\n",
    "    \n",
    "    # pore diameter = 15.12574 from Zeo++ output file\n",
    "    assert X[id_rnd_COF, 0] == 15.12574\n",
    "    \n",
    "    # void fraction = 0.58554 from Zeo++ output file.\n",
    "    assert X[id_rnd_COF, 1] == 0.58554\n",
    "    \n",
    "    # xtal density from Zeo++ output = 0.604869\n",
    "    assert np.isclose(X[id_rnd_COF, 3], 0.604869 * 1000, atol=0.1) # unit convert cuz computed in PM.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd08d5-42b8-4fe2-9f89-86c3f61fe0f9",
   "metadata": {},
   "source": [
    "now that we've tested, normalize the features to lie in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dece51-34b9-4d92-a774-bcdab554bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(X.size()[1]):\n",
    "    X[:, j] = (X[:, j] - torch.min(X[:, j]).item()) / (torch.max(X[:, j]).item() - torch.min(X[:, j]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe57d9-80f5-4dfc-a03a-3a0ca63b621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization worked\n",
    "assert np.allclose(torch.min(X, 0).values, torch.zeros(14))\n",
    "assert np.allclose(torch.max(X, 0).values, torch.ones(14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f6739-5895-4f82-ae86-2739fec319b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert first COF closest to mean\n",
    "if not ablation_study_flag:\n",
    "    x_mean = torch.mean(X, 0)\n",
    "    assert init_cof_ids[0][0] == np.argmin([np.linalg.norm(x_mean - X[j, :].detach().numpy()) for j in range(nb_COFs)])\n",
    "    # assert next COF furthest.\n",
    "    assert init_cof_ids[0][1] == np.argmax([np.linalg.norm(X[init_cof_ids[0][0], :] - X[j, :].detach().numpy()) for j in range(nb_COFs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52c720-2f53-409b-a315-e0442d59950d",
   "metadata": {},
   "source": [
    "print stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e94e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost\n",
    "print(\"total high-fidelity cost:\", sum(cost[1]).item(), \"[min]\")\n",
    "print(\"total low-fidelity cost: \", sum(cost[0]).item(), \"[min]\")\n",
    "print(\"average high-fidelity cost:\", np.mean(cost[1]), \"[min]\")\n",
    "print(\"average low-fidelity cost: \", np.mean(cost[0]), \"[min]\")\n",
    "print(\"average cost ratio:\\t   \", np.mean(cost[1] / cost[0]))\n",
    "\n",
    "# data shape\n",
    "print(\"\\nraw data - \\n\\tX:\", X.shape)\n",
    "for f in range(2):\n",
    "    print(\"\\tfidelity:\", f)\n",
    "    print(\"\\t\\ty:\", y[f].shape)\n",
    "    print(\"\\t\\tcost: \", cost[f].shape)\n",
    "    \n",
    "# normalization check\n",
    "print(\"\\nEnsure features are normalized - \")\n",
    "print(\"max:\\n\", torch.max(X, 0).values)\n",
    "print(\"min:\\n\", torch.min(X, 0).values)\n",
    "print(\"width:\\n\",torch.max(X, 0).values - torch.min(X, 0).values)\n",
    "print(\"mean:\\n\", torch.mean(X, 0))\n",
    "print(\"std:\\n\", torch.std(X, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9ab59",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab7a99",
   "metadata": {},
   "source": [
    "#### Post-Search Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8936ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list of fidelity id's (0's and 1's) from the acquired set.\n",
    "def get_f_ids(acquired_set):\n",
    "    if acquired_set.dim() == 0:\n",
    "        return acquired_set.round().to(dtype=int)\n",
    "    else: \n",
    "        f_ids = [a[0].round().to(dtype=int) for a in acquired_set]\n",
    "        return torch.tensor(f_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of high-fidelity y_max's from iter-to-iter\n",
    "# returns an array.\n",
    "# element i is best y-max high fidelity seen up to iteration i.\n",
    "def get_y_maxes_hf_acquired(acquired_set):    \n",
    "    nb_iters = len(acquired_set)\n",
    "    y_maxes = np.zeros(nb_iters)\n",
    "    # we want the maximum y value (only high-fidelity) up to a given iteration\n",
    "    y_max = 0.0 # update this each iteration.\n",
    "    for i, (f_val, cof_id) in enumerate(acquired_set):\n",
    "        f_id = get_f_ids(torch.tensor(f_val))\n",
    "        assert f_id in [0, 1]\n",
    "        y_acq_this_iter = y[f_id][int(cof_id)]\n",
    "        # i is iteration index\n",
    "        if f_id == 1 and y_acq_this_iter > y_max:  \n",
    "            y_max = y_acq_this_iter # over-write max\n",
    "        y_maxes[i] = y_max \n",
    "    return y_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d16cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find accumulated cost, given acquired set.\n",
    "# returns an array.\n",
    "# element i is cost accumulated till iteration i\n",
    "def accumulated_cost(acquired_set):\n",
    "    nb_iters = len(acquired_set)\n",
    "    accumulated_cost = np.zeros(nb_iters)\n",
    "    for i, (f_val, cof_id) in enumerate(acquired_set):\n",
    "        cof_id = int(cof_id.item())\n",
    "        f_id = f_val.round().to(dtype=int).item()\n",
    "        if i == 0:\n",
    "            accumulated_cost[i] = cost[f_id][cof_id]\n",
    "        else:\n",
    "            accumulated_cost[i] = accumulated_cost[i-1] + cost[f_id][cof_id]\n",
    "    return accumulated_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb35b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte the fraction of sims up to that point that are a given fidelity\n",
    "# entry i is fraction of sims up to that point that are fidelity fidelity.\n",
    "def calc_fidelity_fraction(acquired_set, fidelity):\n",
    "    assert fidelity in [1/3, 2/3] \n",
    "    nb_iters = len(acquired_set)\n",
    "    fid_frac = np.zeros(nb_iters)\n",
    "    for i in range(nb_iters):\n",
    "        fid_frac[i] = sum(acquired_set[:, 0][:i+1] == fidelity) / (i+1)\n",
    "    return fid_frac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cc873",
   "metadata": {},
   "source": [
    "#### constructing initial acquired set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f187575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_acquired_set(initializing_COFs, discrete_fidelities):\n",
    "    return torch.tensor([[f_id, cof_id] for cof_id in initializing_COFs for f_id in discrete_fidelities])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6316e-8a9b-435c-a533-541e9a7835a0",
   "metadata": {},
   "source": [
    "#### building the inputs for the surrogate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341f108-7b34-4480-b8a3-b5e49cbe3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct feature matrix of acquired points.\n",
    "# the last entry is the fidelity parameter.\n",
    "def build_X_train(acquired_set):\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    f_ids = torch.tensor([a[0] for a in acquired_set])\n",
    "    assert f_ids[0] in [1/3, 2/3]\n",
    "    return torch.cat((X[cof_ids, :], f_ids.unsqueeze(dim=-1)), dim=1)\n",
    "\n",
    "# construct output vector for acquired points\n",
    "def build_y_train(acquired_set):\n",
    "    f_ids = get_f_ids(acquired_set)\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    return torch.tensor([y[f_id][cof_id] for f_id, cof_id in zip(f_ids, cof_ids)]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bdc37-6ad8-4526-a025-cf3d30ca3731",
   "metadata": {},
   "source": [
    "#### retreiving costs inccurred for acquired set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603f10f-1896-4d4d-b25d-808ad01be961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vector to track cost of acquired points\n",
    "# entry i is cost of acquired COF i\n",
    "def build_cost(acquired_set):\n",
    "    f_ids = get_f_ids(acquired_set)\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    return torch.tensor([cost[f_id][cof_id] for f_id, cof_id in zip(f_ids, cof_ids)]).unsqueeze(-1)\n",
    "\n",
    "# construct vector to track cost of acquired points\n",
    "# entry i is cost of acquired COF i within a given fidelity_id\n",
    "def build_cost_fidelity(acquired_set, fidelity_id):\n",
    "    assert fidelity_id in [0, 1]\n",
    "    f_ids = get_f_ids(acquired_set)\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    return torch.tensor([cost[f_id][cof_id] for f_id, cof_id in zip(f_ids, cof_ids) if f_id == fidelity_id]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2ef9b",
   "metadata": {},
   "source": [
    "### train surrogate model and retreive its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return trained surrogate model\n",
    "def train_surrogate_model(X_train, y_train):\n",
    "    model = SingleTaskMultiFidelityGP(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        linear_truncated=False, # RBF for features and Downsampling for Fidelities\n",
    "        outcome_transform=Standardize(m=1), # m is the output dimension\n",
    "        data_fidelities=[X_train.shape[1]-1]\n",
    "    )   \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_mll(mll)\n",
    "    return model\n",
    "\n",
    "# return mean, standard deviation of posterior acc to surrogate model\n",
    "def mu_sigma(model, X, fidelity):\n",
    "    assert fidelity in [1/3, 2/3]\n",
    "    nb_COFs_here = X.size()[0]\n",
    "    f = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs_here, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    f_posterior = model.posterior(X_f)\n",
    "    return f_posterior.mean.squeeze().detach().numpy(), np.sqrt(f_posterior.variance.squeeze().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a87aeb",
   "metadata": {},
   "source": [
    "#### Acquisition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5b6473-a35c-4f00-b415-3806d3e5a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of mean cost of sims of high fidelity to those of fidelity-fidelity, within acquired set so far.\n",
    "def estimate_cost_ratio(acquired_set, fidelity):\n",
    "    assert fidelity in [1/3, 2/3]\n",
    "    f_id = get_f_ids(torch.tensor(fidelity))\n",
    "    avg_cost_f  = torch.mean(build_cost_fidelity(acquired_set, f_id))\n",
    "    avg_cost_hf = torch.mean(build_cost_fidelity(acquired_set, 1))\n",
    "    cr = avg_cost_hf / avg_cost_f\n",
    "    return cr.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9f446-bf1b-4c31-992c-0cbbc811f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current best y-value of desired_fidelity in the acquired set\n",
    "def get_y_max(acquired_set, fidelity):\n",
    "    assert fidelity in [0, 1]\n",
    "    f_ids = get_f_ids(acquired_set)\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    return np.max([y[f_id][cof_id] for f_id, cof_id in zip(f_ids, cof_ids) if f_id == fidelity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  efficient multi-fidelity correlation function\n",
    "#  corr(y at given fidelity, y at high-fidelity)\n",
    "#  (see notes)\n",
    "###\n",
    "def mfbo_correlation_function(model, X, fidelity):\n",
    "    assert fidelity in [1/3, 2/3]\n",
    "    # given fidelity\n",
    "    f   = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    \n",
    "    #  high-fidelity\n",
    "    hf   = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * discrete_fidelities[-1]\n",
    "    X_hf = torch.cat((X, hf), dim=1) # last col is associated fidelity\n",
    "\n",
    "    # combine into a single tensor\n",
    "    X_all_fid = torch.cat((X_f, X_hf), dim=0)\n",
    "    \n",
    "    # get variance for each fidelity\n",
    "    var_f = torch.flatten(model.posterior(X_f).variance)\n",
    "    var_hf = torch.flatten(model.posterior(X_hf).variance) # variance\n",
    "    \n",
    "    # posterior covariance \n",
    "    cov = torch.diag(model(X_all_fid).covariance_matrix[:X_f.size()[0], X_f.size()[0]:])\n",
    "    \n",
    "    corr = cov / (torch.sqrt(var_f) * torch.sqrt(var_hf))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b278ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###CHANGE HERE ONLY \n",
    "#EI_hf is UCB now\n",
    "\n",
    "def EI_hf(model, X, acquired_set):\n",
    "    hf_mu, hf_sigma = mu_sigma(model, X, discrete_fidelities[-1])\n",
    "    \n",
    "    # Upper Confidence Bound (UCB)\n",
    "    beta = 25  # exploration parameter - adjust as needed\n",
    "    ucb = hf_mu + np.sqrt(beta) * hf_sigma\n",
    "    return np.maximum(ucb, np.zeros(X.size()[0]))\n",
    "\n",
    "\n",
    "###\n",
    "#  acquisition function\n",
    "###\n",
    "def acquisition_scores(model, X, fidelity, acquired_set):\n",
    "    assert fidelity in [1/3, 2/3]\n",
    "    # expected improvement for high-fidelity\n",
    "    ei = EI_hf(model, X, acquired_set) \n",
    "    \n",
    "    # augmenting functions\n",
    "    corr_f1_f0 = mfbo_correlation_function(model, X, fidelity)\n",
    "    \n",
    "    cr = estimate_cost_ratio(acquired_set, fidelity)\n",
    "\n",
    "    scores = torch.from_numpy(ei) * corr_f1_f0 * cr\n",
    "    return scores.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795dbcc5-fff5-42ca-a9d6-7576f696efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return True if (f_id, cof_id) in acquired set and False otherwise\n",
    "def in_acquired_set(f_id, cof_id, acquired_set):\n",
    "    assert f_id in [0, 1]\n",
    "    fidelity = discrete_fidelities[f_id]\n",
    "    for this_fidelity, this_cof_id in acquired_set:\n",
    "        if this_cof_id == cof_id and this_fidelity == fidelity:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec40fbc",
   "metadata": {},
   "source": [
    "#### tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd92850-307a-4342-995b-f2be7993916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most of these functions operate on an acquired_set which will look like this:\n",
    "bogus_acquired_set = torch.tensor([[2/3, 10], [1/3, 3], [1/3, 4], [2/3, 50]])\n",
    "\n",
    "###\n",
    "#   in_acquired_set\n",
    "###\n",
    "assert not in_acquired_set(0, 10, bogus_acquired_set)\n",
    "assert in_acquired_set(1, 10, bogus_acquired_set)\n",
    "assert in_acquired_set(1, 50, bogus_acquired_set)\n",
    "assert in_acquired_set(0, 3, bogus_acquired_set)\n",
    "assert not in_acquired_set(0, 13, bogus_acquired_set)\n",
    "\n",
    "###\n",
    "#   build_X_train\n",
    "###\n",
    "bogus_X_train = build_X_train(bogus_acquired_set)\n",
    "\n",
    "# first 14 are features.\n",
    "np.allclose(X[[10, 3, 4, 50], :], bogus_X_train[:, :14])\n",
    "\n",
    "# last one is fidelity param\n",
    "assert np.allclose(bogus_X_train[:, 14], [2/3, 1/3, 1/3, 2/3])\n",
    "\n",
    "###\n",
    "#   build_y_train\n",
    "###\n",
    "bogus_y_train = build_y_train(bogus_acquired_set)\n",
    "assert bogus_y_train[0] == y[1][10] # y[fid_id][cof_id]\n",
    "assert bogus_y_train[1] == y[0][3]\n",
    "assert bogus_y_train[2] == y[0][4]\n",
    "assert bogus_y_train[3] == y[1][50]\n",
    "                                   \n",
    "###\n",
    "#   get_f_ids\n",
    "###\n",
    "assert np.array_equal(get_f_ids(bogus_acquired_set), torch.tensor([1, 0, 0, 1]))\n",
    "\n",
    "###\n",
    "#  get_y_maxes_hf_acquired, get_y_max\n",
    "###\n",
    "assert np.all(get_y_maxes_hf_acquired(bogus_acquired_set) == np.array([y[1][10].item(), y[1][10].item(), y[1][10].item(), y[1][50].item()]))\n",
    "assert get_y_max(bogus_acquired_set, 1) == y[1][50]\n",
    "assert get_y_max(bogus_acquired_set[:2], 1) == y[1][10]\n",
    "assert get_y_max(bogus_acquired_set[:], 0) == y[0][3]\n",
    "\n",
    "###\n",
    "#   accumulated_cost, build_cost, build_cost_fidelity\n",
    "###\n",
    "assert np.allclose(build_cost(bogus_acquired_set).squeeze(), np.array([cost[1][10], cost[0][3], cost[0][4], cost[1][50]]))\n",
    "assert np.all(accumulated_cost(bogus_acquired_set) == np.array([cost[1][10], cost[1][10]+cost[0][3], cost[1][10]+cost[0][3]+cost[0][4], cost[1][10]+cost[0][3]+cost[0][4]+cost[1][50]]))\n",
    "assert np.allclose(build_cost_fidelity(bogus_acquired_set, 1).squeeze(), np.array([cost[1][10], cost[1][50]]))\n",
    "assert np.allclose(build_cost_fidelity(bogus_acquired_set, 0).squeeze(), np.array([cost[0][3], cost[0][4]]))\n",
    "\n",
    "\n",
    "###\n",
    "#   estimate_cost_ratio\n",
    "###\n",
    "assert estimate_cost_ratio(bogus_acquired_set, 2/3) == 1 # high to high\n",
    "assert estimate_cost_ratio(bogus_acquired_set, 1/3) > 1# high to low\n",
    "assert estimate_cost_ratio(bogus_acquired_set, 1/3) == (cost[1][10] + cost[1][50]) / (cost[0][3] + cost[0][4])\n",
    "\n",
    "###\n",
    "#   calc_fidelity_fraction\n",
    "###\n",
    "assert np.allclose(calc_fidelity_fraction(bogus_acquired_set, 1/3), np.array([0.0, 1/2, 2/3, 2/4]))\n",
    "assert np.allclose(calc_fidelity_fraction(bogus_acquired_set, 2/3), np.array([1.0, 1/2, 1/3, 2/4]))\n",
    "\n",
    "###\n",
    "#   EI_hf\n",
    "###\n",
    "bogus_X_train_hf = torch.clone(bogus_X_train) # all need to be hf\n",
    "bogus_X_train_hf[1][-1] = 2/3\n",
    "bogus_X_train_hf[2][-1] = 2/3\n",
    "\n",
    "# train bogus model\n",
    "bogus_model = train_surrogate_model(bogus_X_train, bogus_y_train) \n",
    "\n",
    "# use BO Torch's acquisition functions\n",
    "bot_ucb = UpperConfidenceBound(bogus_model, beta=25)\n",
    "bot_ucb_vals = bot_ucb.forward(bogus_X_train_hf.unsqueeze(1))\n",
    "\n",
    "# ours\n",
    "our_ucb = EI_hf(bogus_model, X[[10, 3, 4, 50], :], bogus_acquired_set)\n",
    "assert np.allclose(bot_ucb_vals.detach().numpy(), our_ucb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628918d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug: print the values to see what's different, written by AI\n",
    "# print(\"BoTorch UCB values:\", bot_ei_vals.detach().numpy())\n",
    "# print(\"Our UCB values:\", our_ei)\n",
    "# print(\"Difference:\", bot_ei_vals.detach().numpy() - our_ei)\n",
    "# print(\"Shape of BoTorch output:\", bot_ei_vals.shape)\n",
    "# print(\"Shape of our output:\", our_ei.shape if hasattr(our_ei, 'shape') else type(our_ei))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b84cbe",
   "metadata": {},
   "source": [
    "### Bayesian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae82c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_Bayesian_optimization(nb_iterations, initializing_COFs, verbose=False, stop_after_top_acquired=True):\n",
    "    assert nb_iterations > len(initializing_COFs)\n",
    "    ###\n",
    "    #  initialize acquired set\n",
    "    ###\n",
    "    acquired_set = initialize_acquired_set(initializing_COFs, discrete_fidelities)\n",
    "    \n",
    "    ###\n",
    "    #  analyze-plan-simulate iterations\n",
    "    ###\n",
    "    for i in range(nb_COFs_initialization * len(discrete_fidelities), nb_iterations): \n",
    "        print(\"BO iteration: \", i)\n",
    "        ###\n",
    "        #  construct training data (perform experiments)\n",
    "        ###\n",
    "        X_train = build_X_train(acquired_set)\n",
    "        y_train = build_y_train(acquired_set)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Initialization - \\n\")\n",
    "            print(\"\\tCOF IDs acquired    = \", [acq_[1].item() for acq_ in acquired_set])\n",
    "            print(\"\\tfidelities acquired = \", [acq_[0].item() for acq_ in acquired_set])\n",
    "            print(\"\\tcosts acquired      = \", build_cost(acquired_set), \" [min]\")\n",
    "\n",
    "            print(\"\\n\\tTraining data:\\n\")\n",
    "            print(\"\\t\\t X train shape = \", X_train.shape)\n",
    "            print(\"\\t\\t y train shape = \", y_train.shape)\n",
    "            print(\"\\t\\t training feature vector = \\n\", X_train)\n",
    "        \n",
    "        ###\n",
    "        #  train Model\n",
    "        ###\n",
    "        model = train_surrogate_model(X_train, y_train)\n",
    "        \n",
    "        ###\n",
    "        #  acquire new (COF, fidelity) not yet acquired.\n",
    "        ###\n",
    "        # entry (fid_id, cof_id) is the acquisition value for fidelity f_id and cof cof_id\n",
    "        the_acquisition_scores = np.array([acquisition_scores(model, X, fidelity, acquired_set) for fidelity in discrete_fidelities])\n",
    "        \n",
    "        # overwrite acquired COFs/fidelities with negative infinity to not choose these.\n",
    "        for fidelity, cof_id in acquired_set:\n",
    "            the_acquisition_scores[get_f_ids(fidelity), cof_id.to(dtype=int)] = - np.inf\n",
    "        \n",
    "        # select COF/fidelity with highest aquisition score.\n",
    "        f_id, cof_id = np.unravel_index(np.argmax(the_acquisition_scores), np.shape(the_acquisition_scores))\n",
    "        assert f_id in [0, 1]\n",
    "        assert not in_acquired_set(f_id, cof_id, acquired_set)\n",
    "        assert np.max(the_acquisition_scores) == the_acquisition_scores[f_id, cof_id]\n",
    "        \n",
    "        # update acquired_set\n",
    "        acq = torch.tensor([[discrete_fidelities[f_id], cof_id]]) # dtype=int\n",
    "        acquired_set = torch.cat((acquired_set, acq))\n",
    "\n",
    "        ###\n",
    "        #  print useful info\n",
    "        ###\n",
    "        if verbose:\n",
    "            print(\"\\tacquired COF \", cof_id, \" at fidelity, \", f_id)\n",
    "            print(\"\\t\\ty = \", y[f_id][cof_id].item())\n",
    "            print(\"\\t\\tcost = \", cost[f_id][cof_id])\n",
    "            \n",
    "        if stop_after_top_acquired:\n",
    "            cof_id_with_max_selectivity = np.argmax(y[1])\n",
    "            if cof_id_with_max_selectivity == cof_id and f_id == 1:\n",
    "                print(\"found top COF! exiting.\")\n",
    "                return acquired_set\n",
    "        \n",
    "    return acquired_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130854c",
   "metadata": {},
   "source": [
    "# Run MFBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  construct initial inputs\n",
    "###\n",
    "nb_COFs_initialization = 3   # at each fidelity, number of COFs to initialize with\n",
    "nb_iterations = 2 * nb_COFs  # BO budget, includes initializing COFs. this is actually max # iterations\n",
    "\n",
    "# if ablation_study_flag:\n",
    "#     print(\"ablation study: {}\".format(ablation_study_flag))\n",
    "#     # the maximum possible number itterations = num_fidelities * nb_COFs\n",
    "#     # this would efectively constitute a low-fidelity exhaustive search \n",
    "#     # followed by a high-fidelity exhaustive search\n",
    "#     nb_iterations = 2 * nb_COFs \n",
    "#     print(\"max. number of iterations: {}\".format(nb_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65bd87",
   "metadata": {},
   "source": [
    "run once, using `init_cof_ids[0]`, which is the COF closest to the mean of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf80b8-f37d-4001-8b48-ee4495539fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not ablation_study_flag:\n",
    "    acquired_set = run_Bayesian_optimization(nb_iterations, init_cof_ids[0], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271083c-c9f4-4780-823f-c77ff3791b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack search results\n",
    "f_ids   = [acquired_set[i][0].item()      for i in range(len(acquired_set))]\n",
    "cof_ids = [int(acquired_set[i][1].item()) for i in range(len(acquired_set))]\n",
    "\n",
    "# which COF has the largest high-fidelity selectivity?\n",
    "cof_id_with_max_hi_fid_selectivity = np.argmax(y[1]).item()\n",
    "\n",
    "# iteration we found top COF\n",
    "n_iter_top_cof_found = np.where([cof_ids[i] == cof_id_with_max_hi_fid_selectivity and f_ids[i] > 0.5 for i in range(len(cof_ids))])[0].item()\n",
    "n_iter_top_cof_found \n",
    "\n",
    "#this is the iteration no. in BO run where the top COF was observed at high fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a529d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COF index of the top-performing COF \n",
    "\n",
    "cof_id_with_max_hi_fid_selectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ded64-5ad7-4671-9c1b-6d04d568110e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### observe status of surrogate model's knowledge the iteration before the top COF was acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find COFs that are simualted in with high- and low-fidelity.\n",
    "hi_fid_cofs = [cof_ids[i] for i in range(n_iter_top_cof_found) if f_ids[i] > 0.5]\n",
    "lo_fid_cofs = [cof_ids[i] for i in range(n_iter_top_cof_found) if f_ids[i] < 0.5]\n",
    "# find COFs simulated at both fidelities\n",
    "ids_cofs_hi_and_lo_fid = np.intersect1d(hi_fid_cofs, lo_fid_cofs)\n",
    "ids_cofs_hi_and_lo_fid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96a60a-3404-4302-a125-848393a766c1",
   "metadata": {},
   "source": [
    "the correlation between high- and low-fidelity selectivities. only pertains to those COFs with both simulated hi and lo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7415a9f-fac3-4beb-ae9b-bab536ddc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build selectivity array for plotting, \n",
    "y_los = [y[0][c].item() for c in ids_cofs_hi_and_lo_fid]\n",
    "y_his = [y[1][c].item() for c in ids_cofs_hi_and_lo_fid]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([0, 25], [0, 25], linestyle=\"--\", color=\"k\", linewidth=1)\n",
    "plt.scatter(y_los, y_his, zorder=10)\n",
    "ax = plt.gca()\n",
    "plt.xlim(0, 25)\n",
    "plt.ylim(0, 25)\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "plt.xlabel(\"low-fidelity Xe/Kr selectivity\")\n",
    "plt.ylabel(\"high-fidelity Xe/Kr selectivity\")\n",
    "plt.tight_layout()\n",
    "if not ablation_study_flag:\n",
    "    plt.savefig(\"../figs/lo_vs_hi_fid_selectivity.pdf\", format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a7d1d-3176-4261-94ed-d173e38ee43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get COF ids not in acquired set with high-fidelity sims. these are test COFs for high-fidelity standpoint.\n",
    "test_cof_ids = [cof_id for cof_id in range(nb_COFs) if not (cof_id in hi_fid_cofs)]\n",
    "len(test_cof_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a62f77-432e-42a3-ad25-cfcd82dab491",
   "metadata": {},
   "outputs": [],
   "source": [
    "cof_id_with_max_hi_fid_selectivity in test_cof_ids # the COF with the highest selectivity should be in here. cuz we didn't acquire it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e707b-8a68-4277-acbb-a806bba28162",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_in_test_cofs_of_opt_cof = np.where([c == cof_id_with_max_hi_fid_selectivity for c in test_cof_ids])[0].item()\n",
    "id_in_test_cofs_of_opt_cof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7572f-ed5a-45e0-b00a-e6e264549ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train surrogate model for test data, on acquired set up till top COF was found.\n",
    "X_train = build_X_train(acquired_set[:n_iter_top_cof_found])\n",
    "y_train = build_y_train(acquired_set[:n_iter_top_cof_found])\n",
    "\n",
    "X_test = X[test_cof_ids, :]\n",
    "\n",
    "model = train_surrogate_model(X_train, y_train)\n",
    "\n",
    "# get model predictions on test COFs, for high-fidelity.\n",
    "y_pred, sigma = mu_sigma(model, X_test, discrete_fidelities[-1])\n",
    "\n",
    "# plot true vs predicted\n",
    "y_true = [y[1][c].item() for c in test_cof_ids]\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "abserr = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "###\n",
    "#  parity plot\n",
    "###\n",
    "gridspec_kw={'width_ratios': [6, 2], 'height_ratios': [2, 6]} # set ratios\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, gridspec_kw=gridspec_kw, figsize=(8, 8))\n",
    "# fig = plt.figure()\n",
    "ax[0, 1].axis(\"off\")\n",
    "ax[1, 0].plot([0, 20], [0, 20], linestyle=\"--\", color=\"k\", linewidth=1)\n",
    "# ax = plt.gca()\n",
    "ax[1, 0].set_xlim(0, 20)\n",
    "ax[1, 0].set_ylim(0, 20)\n",
    "\n",
    "#ax[1, 0].set_aspect(\"equal\", \"box\")\n",
    "\n",
    "ax[1, 0].text(5, 15, \"R$^2$=%.2f\\nMAE=%.2f\" % (r2, abserr))\n",
    "ax[1, 0].scatter(y_true, y_pred, fc='none', ec=\"k\")\n",
    "ax[1, 0].set_xlabel(\"true\\nhigh-fidelity Xe/Kr selectivity\")\n",
    "ax[1, 0].set_ylabel(\"predicted\\nhigh-fidelity Xe/Kr selectivity\")\n",
    "# plot acquired COF\n",
    "ax[1, 0].scatter(y_true[id_in_test_cofs_of_opt_cof], y_pred[id_in_test_cofs_of_opt_cof], marker=\"x\", color=\"red\")\n",
    "\n",
    "\n",
    "###\n",
    "#  histogram of selectivities\n",
    "###\n",
    "hist_color = sns.color_palette(\"husl\", 8)[7]\n",
    "ax[0, 0].hist(y_true, color=hist_color, alpha=0.5) # \n",
    "ax[0, 0].sharex(ax[1, 0])\n",
    "ax[0, 0].set_ylabel('# COFs')\n",
    "plt.setp(ax[0, 0].get_xticklabels(), visible=False) # remove yticklabels\n",
    "\n",
    "hist_color = sns.color_palette(\"husl\", 8)[7]\n",
    "ax[1, 1].hist(y_pred, color=hist_color, alpha=0.5, orientation=\"horizontal\") # \n",
    "ax[1, 1].sharey(ax[1, 0])\n",
    "ax[1, 1].set_xlabel('# COFs')\n",
    "plt.setp(ax[1, 1].get_yticklabels(), visible=False) # remove yticklabels\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "if not ablation_study_flag:\n",
    "    plt.savefig(\"../figs/surrogate_parity_with_hist.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c38b78c-a377-45a7-8710-713b2ddcbdc3",
   "metadata": {},
   "source": [
    "for kicks, compare to RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4874a-aede-404f-9e32-cccd20f17ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor()\n",
    "# rf.fit(X_train[:, :14], y_train)\n",
    "# rf.score(X_test, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60c44d-e85a-4705-b475-c59d9dde4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_sorted = np.argsort(y_true)[::-1]\n",
    "\n",
    "# plt.figure(figsize=(12, 3))\n",
    "# plt.errorbar(range(len(y_true)), y_pred[ids_sorted], yerr=sigma, linewidth=1, marker=\"o\")\n",
    "# plt.errorbar(0, y_pred[id_in_test_cofs_of_opt_cof], yerr=sigma[id_in_test_cofs_of_opt_cof], linewidth=1, marker=\"o\", color=\"red\")\n",
    "# plt.xlabel(\"rank according to true high-fidelity Xe/Kr selectivity\")\n",
    "# plt.ylabel(\"predicted\\nhigh-fidelity\\nXe/Kr selectivity\")\n",
    "# plt.xlim(-1, len(test_cof_ids) +1)\n",
    "# plt.tight_layout()\n",
    "# if not ablation_study_flag:\n",
    "#     plt.savefig(\"../figs/surrogate_predictions.pdf\", format=\"pdf\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47061e4c-bce9-49bd-99d1-d9bbf1a64040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where([cof_id == cof_id_with_max_hi_fid_selectivity for cof_id in test_cof_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b443dd-a64c-4733-b888-0eab86a44719",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run MFBO under different initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cff5e7-bc77-4802-9bfb-5620b7c00db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run_results(acquired_set, run, ablation_study_flag):\n",
    "    # compute attributes of acquired set\n",
    "    y_acquired    = build_y_train(acquired_set)\n",
    "    y_maxes_acq   = get_y_maxes_hf_acquired(acquired_set.detach().numpy())\n",
    "    fid_fraction  = calc_fidelity_fraction(acquired_set.detach().numpy(), discrete_fidelities[1])\n",
    "    cost_acquired = build_cost(acquired_set)\n",
    "    acc_cost      = accumulated_cost(acquired_set)\n",
    "    \n",
    "    # when did MFBO recover top COF?\n",
    "    cof_id_with_max_selectivity = np.argmax(y[1])\n",
    "    BO_iter_top_cof_acquired = float(\"inf\") # dummy \n",
    "    for i, (f_id, cof_id) in enumerate(acquired_set):\n",
    "        if cof_id.to(dtype=int) == cof_id_with_max_selectivity and get_f_ids(f_id) == 1:\n",
    "            BO_iter_top_cof_acquired = i\n",
    "            break\n",
    "        elif i == len(acquired_set) - 1:\n",
    "            print(\"oh no, top COF not acquired!\")\n",
    "    \n",
    "    mfbo_res = dict({\n",
    "        'acquired_set': acquired_set.detach().numpy(),\n",
    "         'y_acquired': y_acquired.detach().numpy(),\n",
    "         'y_max_acquired': y_maxes_acq,\n",
    "         'fidelity_fraction': fid_fraction,\n",
    "         'cost_acquired': cost_acquired.flatten().detach().numpy(),\n",
    "         'accumulated_cost': acc_cost / 60,\n",
    "         'nb_COFs_initialization': nb_COFs_initialization,\n",
    "         'BO_iter_top_cof_acquired': BO_iter_top_cof_acquired,\n",
    "         'elapsed_time (min)': 0.0,\n",
    "         'post_preprint': True\n",
    "        })\n",
    "    \n",
    "    import os  \n",
    "    import pickle\n",
    "    os.makedirs('../search_results/mfbo/Run5_10Dec_UCB_B25', exist_ok=True)\n",
    "\n",
    "    pickle_filename = '../search_results/mfbo/Run5_10Dec_UCB_B25/mfbo_results_run_{}'.format(run)\n",
    "    if ablation_study_flag:\n",
    "        pickle_filename += \"_ablation\"\n",
    "    pickle_filename += \".pkl\"\n",
    "    with open(pickle_filename, 'wb') as file:\n",
    "        pickle.dump(mfbo_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66fd66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "#  run search\n",
    "###\n",
    "for j in range(nb_runs):\n",
    "    initializing_COFs = init_cof_ids[j]\n",
    "    if ablation_study_flag:\n",
    "        # each time, randomly shuffle features within a column\n",
    "        for k in range(X.size()[1]):\n",
    "            row_ids = torch.randperm(X.size()[0])\n",
    "            X[:, k] = X[row_ids, k]\n",
    "\n",
    "    # check the length of each initializing set\n",
    "    assert len(initializing_COFs) == nb_COFs_initialization\n",
    "    print(\"run #: {}\".format(j))\n",
    "\n",
    "    ###\n",
    "    #  run BO search\n",
    "    ###\n",
    "    acquired_set = run_Bayesian_optimization(nb_iterations, initializing_COFs)\n",
    "    save_run_results(acquired_set, j, ablation_study_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79fbe7-ba43-4717-bfe6-526b734b7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_runs"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "873b9eed737f4a6d896d154d73024f53",
   "lastKernelId": "464ee535-3ea4-4199-a625-569df1ee6433"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
